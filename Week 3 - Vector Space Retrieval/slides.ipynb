{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Mining\n",
    "\n",
    "# Lecture 3: Vector Space Retrieval \n",
    "\n",
    "![img](https://3.bp.blogspot.com/_tOOi3R89e74/TUeyueig7ZI/AAAAAAAAAJQ/QHL-VLEWook/s1600/vector_space.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recap\n",
    "\n",
    "Data Collection -> Information Selection -> Pre-processing -> ??? -> Classification\n",
    "\n",
    "Plain Text -> Tokens -> Vocabulary -> 'Word' Vectors\n",
    "\n",
    "Word - Token - Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Document to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'text', 'this', 'is']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = \"this is a text this is\"\n",
    "tokens = document.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'is', 'text', 'this'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = set(tokens)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# warning, this example is simple but actually runs pretty slow\n",
    "# on real data\n",
    "document_vector = [tokens.count(term) for term in sorted(vocabulary)]\n",
    "document_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Documents to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'and', 'another', 'here', 'is', 'text', 'this'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"this is a text this is\", \"and here is another text\"]\n",
    "doc_tokens = [document.split() for document in documents]\n",
    "vocabulary = set([term for tokens in doc_tokens for term in tokens])\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 2, 1, 2],\n",
       "        [0, 1, 1, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "document_vectors = [[tokens.count(term) for term in sorted(vocabulary)]\n",
    "                    for tokens in doc_tokens]\n",
    "np.matrix(document_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 2, 1, 2],\n",
       "        [1, 1, 1, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vectors = cv.fit_transform(documents)\n",
    "document_vectors.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0, 'another': 1, 'here': 2, 'is': 3, 'text': 4, 'this': 5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Boolean Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = \"text about stuff\"\n",
    "B = \"stuff about text\"\n",
    "C = \"text about vectors\"\n",
    "D = \"vectors are handy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which document is D most similar to?\n",
    "\n",
    "| doc  | about | are | handy | stuff | text | vectors |\n",
    "| ---- | ----- | --- | ----- | ----- | ---- | ------- |\n",
    "| A    | 1     |     |       | 1     | 1    |         |\n",
    "| B    | 1     |     |       | 1     | 1    |         |\n",
    "| C    | 1     |     |       |       | 1    | 1       |\n",
    "| D    |       | 1   | 1     |       |      | 1       |\n",
    "\n",
    "- `Document * Term` or `Term * Document` Matrix\n",
    "- Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Jaccard Coefficient - $J(A, B) = \\frac{| A \\cap B |}{| A \\cup B |}$\n",
    "\n",
    "| doc  | about | are | handy | stuff | text | vectors |\n",
    "| ---- | ----- | --- | ----- | ----- | ---- | ------- |\n",
    "| A    | 1     |     |       | 1     | 1    |         |\n",
    "| B    | 1     |     |       | 1     | 1    |         |\n",
    "| C    | 1     |     |       |       | 1    | 1       |\n",
    "| D    |       | 1   | 1     |       |      | 1       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Ds: 0.0\n",
      "As Bs: 1.0\n",
      "Cs Ds: 0.2\n"
     ]
    }
   ],
   "source": [
    "As = set(A.split())\n",
    "Bs = set(B.split())\n",
    "Cs = set(C.split())\n",
    "Ds = set(D.split())\n",
    "\n",
    "print(\"As Ds:\", len(As & Ds) / len(As | Ds))\n",
    "print(\"As Bs:\", len(As & Bs) / len(As | Bs))\n",
    "print(\"Cs Ds:\", len(Cs & Ds) / len(Cs | Ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Order\n",
    "- Context\n",
    "- Information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Term Frequencies - $tf_{t,d}$\n",
    "\n",
    "t = term, d = document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x5361 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9245 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "cv = CountVectorizer()\n",
    "documents = {doc: open(doc).read() for doc in glob('../Week 1 - Introduction/data/*.txt')}\n",
    "D = cv.fit_transform(documents.values())\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text-mining.txt', 'artificial-intelligence.txt', 'information-retrieval.txt', 'machine-learning.txt', 'natural-language-processing.txt', 'computer-vision.txt']\n"
     ]
    }
   ],
   "source": [
    "print([x.replace('../Week 1 - Introduction/data/', '') for x in documents.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2970"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_['learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  6],\n",
       "        [ 46],\n",
       "        [  2],\n",
       "        [134],\n",
       "        [ 27],\n",
       "        [ 10]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[:,2970].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Issues\n",
    "\n",
    "- Frequency 10 is more important than 100, but also * 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.84509804],\n",
       "        [ 1.67209786],\n",
       "        [ 0.47712125],\n",
       "        [ 2.13033377],\n",
       "        [ 1.44715803],\n",
       "        [ 1.04139269]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(D.todense() + 1)[:,2970]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Longer documents shouldn't be more likely to be interesting.\n",
    "- Rare terms should be informative (amongst all documents).\n",
    "    - If D1 and D2 both have 'cross-validation' in their vectors, and all the other documents don't, that should be a very strong similarity indication.\n",
    "\n",
    "Latter: document frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Document Frequency - $idf_t = \\log_{10} (N / df_{t})$\n",
    "\n",
    "Number of documents that contain some term. Inverse measure of informativeness.\n",
    "\n",
    "* N = number of documents\n",
    "* $df_t$ = document frequency of term $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.47712125472\n",
      "0.778151250384\n"
     ]
    }
   ],
   "source": [
    "def idf(term):\n",
    "    return np.log10(len(documents) / sum([term in d.split() for d in documents.values()]))\n",
    "\n",
    "print(idf(\"learning\"))\n",
    "print(idf(\"parsing\"))\n",
    "print(idf(\"naive\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TF\\*IDF Weighting\n",
    "\n",
    "Also tf-idf, tf.idf, etc.\n",
    "\n",
    "$w_{t,d} = (1 + \\log tf_{t,d} ) \\cdot \\log_{10} (N / df_{t})$\n",
    "\n",
    "Increases with rarity and occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31730563493470437"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tokens = list(documents.values())[0].split()\n",
    "(np.log(document_tokens.count(\"text\") + 1)) * idf(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.01783667],\n",
       "        [ 0.05364644],\n",
       "        [ 0.00425381],\n",
       "        [ 0.36599158],\n",
       "        [ 0.06741781],\n",
       "        [ 0.02036652]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "D = tfidf.fit_transform(documents.values())\n",
    "D.todense()[:,2970]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TF\\*IDF Weighting II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text-mining.txt', 'artificial-intelligence.txt', 'information-retrieval.txt', 'machine-learning.txt', 'natural-language-processing.txt', 'computer-vision.txt']\n"
     ]
    }
   ],
   "source": [
    "print([x.replace('../Week 1 - Introduction/data/', '') for x in documents.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4882"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.3431035 ],\n",
       "        [ 0.001346  ],\n",
       "        [ 0.03436678],\n",
       "        [ 0.        ],\n",
       "        [ 0.063401  ],\n",
       "        [ 0.00705181]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.todense()[:,4882]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we know if they are similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector Space Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Vectors in Space\n",
    "\n",
    "- Features\n",
    "- Points\n",
    "\n",
    "Below: sentence = document, term = word\n",
    "\n",
    "* term 1 = \"a\"\n",
    "* term 2 = \"sentence\"\n",
    "* term n = \"that\"\n",
    "\n",
    "\n",
    "* sentence 1 = \"a that\"\n",
    "* sentence 2 = \"that sentence\"\n",
    "* sentence n = \"a sentence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![img](https://3.bp.blogspot.com/_tOOi3R89e74/TUeyueig7ZI/AAAAAAAAAJQ/QHL-VLEWook/s1600/vector_space.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Common Display ... Mining\n",
    "\n",
    "- 2-D\n",
    "- Vector endpoints\n",
    "- Vectors have a label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img2](http://nlp.stanford.edu/IR-book/html/htmledition/img1087.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Common Display Information Retrieval\n",
    "\n",
    "- 2D\n",
    "- Full vectors\n",
    "- q = query\n",
    "\n",
    "![img3](https://d2qphtmbcjv60w.cloudfront.net/content/jaminfo/9/6/637/F4.large.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Similarity in Vector Space Models\n",
    "\n",
    "- First need to normalize all vectors (L2 norm or Eucledian norm, results in a unit vector).\n",
    "\n",
    "### $|| \\vec{x} ||_{2} = \\sqrt{\\sum_i x_{i}^{2}}$\n",
    "= dividing all vector points by its length (sum)\n",
    "\n",
    "![img4](https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/vector_example.png)\n",
    "\n",
    "So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "v1 = [-1, 3, 5]\n",
    "v2 = [2, -3, 20]\n",
    "\n",
    "v1_2 = [x / np.sqrt(sum(v1)**2) for x in v1]\n",
    "v2_2 = [x / np.sqrt(sum(v2)**2) for x in v2]\n",
    "\n",
    "print(v1_2)\n",
    "print(v2_2)\n",
    "print(sum(v1_2), sum(v2_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Similarity in Vector Space Models II\n",
    "\n",
    "Then we can calculate cosine similarity. Inverse of angle; vectors that are 90 deg. apart have a similarity of 0, whilst perfect aligned ones have 1. Given L2, simple dot product:\n",
    "\n",
    "$a \\bullet b = \\sum^{n}_{i=1} a_i b_i = a_1 b_1 + a_2 b_2 + \\ldots + a_n b_n$\n",
    "\n",
    "So:\n",
    "\n",
    "$[0.2, 0.1, 0.7] \\bullet [0.4, 0.2, 0.4] = 0.2 \\cdot 0.4 + 0.1 \\cdot 0.2 + 0.7 \\cdot 0.4  = 0.38$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### So now you know a basic $k$-NN implementation!\n",
    "\n",
    "- The classifier $k$-NN (nearest neighbours) looks at $k$ nearest (with for example cosine distance) vectors.\n",
    "- Fit/train phase: convert all documents to vectors, remember vocab for new transformation.\n",
    "- Test phase: convert given document to same length vector, calculate distance to all known vectors, select top $k$.\n",
    "- If you have labels: look up labels for top $k$, do (for example) majority vote, return label."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
