{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Mining\n",
    "# Lecture 4: Distributed Semantics\n",
    "\n",
    "![img](https://www.tensorflow.org/versions/r0.10/images/linear-relationships.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap & Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Last Week\n",
    "\n",
    "- Text can be tokenized, tokens can be counted.\n",
    "- Documents can be represented as a vector with token count values.\n",
    "- These vectors can be seen as a Vector Space model.\n",
    "- Vectors can be weighted to be more informative.\n",
    "- Through simple Vector Space calculations we can do some classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = np.asarray(vectorizer.fit_transform(newsgroups.data).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3911516433798255,\n",
       " 0.29651994760185252,\n",
       " 0.25581635272977149,\n",
       " 0.24790204776245364,\n",
       " 0.21241493770390013]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = [\"USA space launch\"]\n",
    "qv = np.asarray(vectorizer.transform(query).todense())[0]\n",
    "cos = {}\n",
    "\n",
    "for i, dv in enumerate(vectors):\n",
    "    cos[np.dot(qv, dv)] = i\n",
    "    \n",
    "scores = sorted(cos)[::-1]\n",
    "scores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sci.space'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.target_names[newsgroups.target[cos[scores[0]]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In Practice II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.windows.x'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = [\"blue screen of death\"]\n",
    "qv = np.asarray(vectorizer.transform(query).todense())[0]\n",
    "cos = {}\n",
    "\n",
    "for i, dv in enumerate(vectors):\n",
    "    cos[np.dot(qv, dv)] = i\n",
    "    \n",
    "scores = sorted(cos)[::-1]\n",
    "\n",
    "newsgroups.target_names[newsgroups.target[cos[scores[0]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sci.crypt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = [\"John kicked the ball\"]\n",
    "qv = np.asarray(vectorizer.transform(query).todense())[0]\n",
    "cos = {}\n",
    "\n",
    "for i, dv in enumerate(vectors):\n",
    "    cos[np.dot(qv, dv)] = i\n",
    "    \n",
    "scores = sorted(cos)[::-1]\n",
    "\n",
    "newsgroups.target_names[newsgroups.target[cos[scores[0]]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Left-over Issues\n",
    "\n",
    "- TF\\*IDF works pretty well, but has nothing to do with language other than occurence.\n",
    "- Does not take into account order, context, semantics, nor any properties of the words.\n",
    "- Information properties of both parts tf (Luhn, 1957), and df (Jones, 1972) pretty dated, and lack strong theoretical grounding.\n",
    "- Need a lot of preprocessing to fix noisy features (e.g. misspellings).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fix 1: Change the Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$n$-grams to the rescue!\n",
    "\n",
    "- Up until now: word frequencies and bag-of-words.\n",
    "- What if we use more words as features? - `\"the cat\", \"cat jumped\" \"jumped on\" \"on the\" \"the table\"`.\n",
    "- Do the occurences of these pairs model language well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Language Modelling\n",
    "\n",
    "Very easy approach: Markov Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'am'), ('am', 'Sam.'), ('Sam.', 'Sam'), ('Sam', 'I'), ('I', 'am.'), ('am.', 'I'), ('I', 'do'), ('do', 'not'), ('not', 'like'), ('like', 'green'), ('green', 'eggs'), ('eggs', 'and'), ('and', 'ham.')]\n"
     ]
    }
   ],
   "source": [
    "s = \"I am Sam. Sam I am. I do not like green eggs and ham.\"\n",
    "tokens = s.split()\n",
    "bigrams = list(zip(*[tokens[i:] for i in range(2)]))\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img2](https://sookocheff.com/img/nlp/ngram-modeling-with-markov-chains/transitions-from-I.png)\n",
    "\n",
    "![img3](https://sookocheff.com/img/nlp/ngram-modeling-with-markov-chains/following-transitions-from-I.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class MarkovChain:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.memory = {}\n",
    "\n",
    "    def _learn_key(self, key, value):\n",
    "        if key not in self.memory:\n",
    "            self.memory[key] = []\n",
    "\n",
    "        self.memory[key].append(value)\n",
    "\n",
    "    def learn(self, text):\n",
    "        tokens = text.split(\" \")\n",
    "        bigrams = zip(*[tokens[i:] for i in range(2)])\n",
    "        for bigram in bigrams:\n",
    "            self._learn_key(bigram[0], bigram[1])\n",
    "\n",
    "    def _next(self, current_state):\n",
    "        next_possible = self.memory.get(current_state)\n",
    "\n",
    "        if not next_possible:\n",
    "            next_possible = self.memory.keys()\n",
    "\n",
    "        return random.sample(next_possible, 1)[0]\n",
    "\n",
    "    def babble(self, amount, state=''):\n",
    "        if not amount:\n",
    "            return state\n",
    "\n",
    "        next_word = self._next(state)\n",
    "        return state + ' ' + self.babble(amount - 1, next_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Language Modelling II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "\n",
    "files = glob('../Week 1 - Introduction/data/*.txt')\n",
    "chain = MarkovChain()\n",
    "\n",
    "for f in files:\n",
    "    text = open(f).read().lower()\n",
    "    chain.learn(re.sub('[^\\w \\-]', '', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    text-proofing  disbelief   a label to build a specific problems reasoning using computer vision and vicarious to neural networks were then using guidance space odyssey released as often with restricted blocks worlds with hand-written rules similar to roc curve this with our particular query '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.babble(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```'  this measure of the 1940s alan turings proposal to is strongly np-hard and retrieval and job offers related to the ability to take one or other terms is an algorithm basic techniques should artificial neuronsthe field of artificial beings head and applications include swarm intelligence ai effecthigh-profile examples of'```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Further Expansion\n",
    "\n",
    "- Can use any $n$-gram; trigrams, tetragram, etc.\n",
    "    - The longer the gram, the more improbable it will be in test data.\n",
    "- Use character grams to capture spelling variations.\n",
    "    - Very effective in stylometry.\n",
    "\n",
    "$n$-grams have proven very effective in a lot of text mining applications (and are hard to beat baselines); however, cannot capture long dependencies, or intuitive relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributed Semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
