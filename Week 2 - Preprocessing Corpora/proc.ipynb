{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Excersises\n",
    "\n",
    "**Note**: this week there are no tasks, nothing will be graded. You are free to ask questions regarding these excersises though! What we will do is try to apply some of the code shown in the lectures to kind of get things rolling. As I probably mention in the lecture, it's not important for you to completely understand all the code shown. This will probably make more sense at the end of your Data Processing lectures. For the time being, try to kind of understand what each function is doing, and more importantly, make sure you understand what each component in the pre-processing pipeline does on a higher level.\n",
    "\n",
    "## Retrieving HTML\n",
    "\n",
    "We will be using [this](http://www.imdb.com/title/tt0088763/synopsis?ref_=tt_ql_stry_3) page. In the sheet examples, I could conveniently extract all text because they were neatly put around `<p>...</p>` tags. Sadly, this is not the case for the page we are looking at today. Beautifulsoup (the library that makes HTML searchable), can search for more complex patterns though. What you will see a lot is for example `<div class=\"something\">...</div>`. If you want all `div`s, that as class have `something`, the syntax for that is as follows: `soup.find_all('div', {'class': 'something'})`. Now try to get the synopsis text for the page using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "def url_to_data(url):\n",
    "    \"\"\"Extracts html for a given url.\"\"\"\n",
    "    http = urllib3.PoolManager()\n",
    "    html = http.request('GET', url).data.decode('utf-8')\n",
    "    return html\n",
    "\n",
    "url = '...'  # add\n",
    "soup = bs(url_to_data(url), \"lxml\")\n",
    "results = soup.find_all('div', {})  # add\n",
    "text = [x.text for x in results][0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can mess with `text` here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to apply any of the things we are discussing below to this `text` variable. For now however, we are going to work with another data format, namely JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tell me why McDonald's gave me a sweet tea w no ice?Â¿ #wtffffðŸ™„\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "tweets = pickle.load(open('data/tweets.pickle', 'rb'))\n",
    "tweets[2]['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
